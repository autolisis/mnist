{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reisub0/mnist/blob/master/mnist2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TFRYtvXE6NgT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LwNPUSfF7GXt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://pjreddie.com/media/files/mnist_train.csv\n",
        "!wget https://pjreddie.com/media/files/mnist_test.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GB_nyvGP7MA2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s9HOTkXC7nUo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('mnist_train.csv', header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yh1Qdesd7ydR",
        "colab_type": "code",
        "outputId": "288f43ed-2693-4b3f-bcdc-5f7127a03bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LxJ2BmdW-FtA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# from torchvision import datasets, transforms\n",
        "\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=True, download=True,\n",
        "#                    transform=transforms.Compose([\n",
        "#                        transforms.ToTensor(),\n",
        "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                    ])),\n",
        "#     batch_size=100, shuffle=True, **kwargs)\n",
        "\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "#                        transforms.ToTensor(),\n",
        "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                    ])),\n",
        "#     batch_size=100, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zot6UnBtCVDZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mean = 33.318421449829934\n",
        "sd = 78.56749081851163\n",
        "# mean = 0.1307\n",
        "# sd = 0.3081\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class dataset(Dataset):\n",
        "    def __init__(self, csv, transform=None):\n",
        "        data = pd.read_csv(csv, header=None)\n",
        "        self.X = np.array(data.iloc[:, 1:]).reshape(-1, 28, 28, 1).astype('float32')\n",
        "        self.Y = np.array(data.iloc[:, 0])\n",
        "        \n",
        "        del data\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.X[idx]\n",
        "        label = self.Y[idx]\n",
        "        \n",
        "        if self.transform:\n",
        "            item = self.transform(item)\n",
        "            \n",
        "        return (item, label)\n",
        "      \n",
        "import torchvision.transforms as transforms\n",
        "trainData = dataset('mnist_train.csv', transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean,), (sd,))\n",
        "]))\n",
        "testData = dataset('mnist_test.csv', transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean,), (sd,))\n",
        "]))\n",
        "\n",
        "train_loader = DataLoader(dataset=trainData,\n",
        "                         batch_size=10, \n",
        "                         shuffle=True,\n",
        "                         )\n",
        "test_loader = DataLoader(dataset=testData, \n",
        "                        batch_size=10, \n",
        "                        shuffle=True,\n",
        "                        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h6llXOTHad43",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# mean = 33.318421449829934\n",
        "# sd = 78.56749081851163\n",
        "mean1 = 0.1307\n",
        "sd1 = 0.3081\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class dataset1(Dataset):\n",
        "    def __init__(self, csv, transform=None):\n",
        "        data = pd.read_csv(csv, header=None)\n",
        "        self.X = np.array(data.iloc[:, 1:]).reshape(-1, 28, 28, 1).astype('uint8')\n",
        "        self.Y = np.array(data.iloc[:, 0])\n",
        "        \n",
        "        del data\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.X[idx]\n",
        "        label = self.Y[idx]\n",
        "        \n",
        "        if self.transform:\n",
        "            item = self.transform(item)\n",
        "            \n",
        "        return (item, label)\n",
        "      \n",
        "import torchvision.transforms as transforms\n",
        "trainData1 = dataset1('mnist_train.csv', transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean1,), (sd1,))\n",
        "]))\n",
        "testData1 = dataset1('mnist_test.csv', transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean1,), (sd1,))\n",
        "]))\n",
        "\n",
        "train_loader1 = DataLoader(dataset=trainData1,\n",
        "                         batch_size=10, \n",
        "                         shuffle=True,\n",
        "                         )\n",
        "test_loader1 = DataLoader(dataset=testData1, \n",
        "                        batch_size=10, \n",
        "                        shuffle=True,\n",
        "                        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fOUwif-_TN4I",
        "colab_type": "code",
        "outputId": "51c8f12d-e753-44a4-9058-26eccbcc5984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2186
        }
      },
      "cell_type": "code",
      "source": [
        "print(trainData[0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[[-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.3859, -0.1950,\n",
            "          -0.1950, -0.1950,  1.1796,  1.3069,  1.8033, -0.0931,  1.6888,\n",
            "           2.8215,  2.7197,  1.1924, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.0422,  0.0341,  0.7723,  1.5360,  1.7397,  2.7961,\n",
            "           2.7961,  2.7961,  2.7961,  2.7961,  2.4397,  1.7651,  2.7961,\n",
            "           2.6561,  2.0579,  0.3905, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "           0.1996,  2.6052,  2.7961,  2.7961,  2.7961,  2.7961,  2.7961,\n",
            "           2.7961,  2.7961,  2.7961,  2.7706,  0.7596,  0.6196,  0.6196,\n",
            "           0.2887,  0.0723, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.1950,  2.3633,  2.7961,  2.7961,  2.7961,  2.7961,  2.7961,\n",
            "           2.0961,  1.8924,  2.7197,  2.6434, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241,  0.5942,  1.5615,  0.9378,  2.7961,  2.7961,  2.1851,\n",
            "          -0.2841, -0.4241,  0.1232,  1.5360, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.2459, -0.4113,  1.5360,  2.7961,  0.7214,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241,  1.3451,  2.7961,  1.9942,\n",
            "          -0.3986, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.2841,  1.9942,  2.7961,\n",
            "           0.4669, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,  0.0214,  2.6434,\n",
            "           2.4397,  1.6124,  0.9505, -0.4113, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,  0.6069,\n",
            "           2.6306,  2.7961,  2.7961,  1.0905, -0.1059, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "           0.1487,  1.9433,  2.7961,  2.7961,  1.4851, -0.0804, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.2204,  0.7596,  2.7834,  2.7961,  1.9560, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241,  2.7452,  2.7961,  2.7452,  0.3905,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "           0.1614,  1.2306,  1.9051,  2.7961,  2.7961,  2.2106, -0.3986,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,  0.0723,  1.4597,\n",
            "           2.4906,  2.7961,  2.7961,  2.7961,  2.7579,  1.8924, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.1186,  1.0269,  2.3888,  2.7961,\n",
            "           2.7961,  2.7961,  2.7961,  2.1342,  0.5687, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.1313,  0.4160,  2.2870,  2.7961,  2.7961,  2.7961,\n",
            "           2.7961,  2.0961,  0.6069, -0.3986, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.1950,\n",
            "           1.7524,  2.3633,  2.7961,  2.7961,  2.7961,  2.7961,  2.0579,\n",
            "           0.5942, -0.3095, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241,  0.2760,  1.7651,  2.4524,\n",
            "           2.7961,  2.7961,  2.7961,  2.7961,  2.6815,  1.2687, -0.2841,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241,  1.3069,  2.7961,  2.7961,\n",
            "           2.7961,  2.2742,  1.2942,  1.2560, -0.2204, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241],\n",
            "         [-0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241,\n",
            "          -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241, -0.4241]]]), 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mwa5TLWZYnOe",
        "colab_type": "code",
        "outputId": "49711cc7-7866-4245-d2a7-fa8f4eef6a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2186
        }
      },
      "cell_type": "code",
      "source": [
        "print(trainData1[0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n",
            "          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n",
            "           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n",
            "           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n",
            "           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
            "           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n",
            "           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
            "           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n",
            "          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n",
            "          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n",
            "           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n",
            "           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n",
            "           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n",
            "           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n",
            "           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n",
            "           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n",
            "           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n",
            "           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n",
            "           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n",
            "           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]), 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TtMR6amc_EBI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BtIi0uke_O1d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sRoxjlI6AB2q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GXKpbk2VAPUy",
        "colab_type": "code",
        "outputId": "27406bc7-5ee5-4089-e66f-1ad1f715ab70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2922
        }
      },
      "cell_type": "code",
      "source": [
        "args = {}\n",
        "for epoch in range(1, 10 + 1):\n",
        "    train(args, model, device, train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.283738\n",
            "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.485644\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.376262\n",
            "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.200341\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.390658\n",
            "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.302362\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.027396\n",
            "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.261035\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.309425\n",
            "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.066346\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.069608\n",
            "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.628914\n",
            "\n",
            "Test set: Average loss: 0.0768, Accuracy: 9750/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.340797\n",
            "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 2.228594\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 1.630923\n",
            "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 1.615383\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 1.540557\n",
            "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 1.330777\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 1.008767\n",
            "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 1.699225\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.995483\n",
            "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 1.704101\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.998849\n",
            "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 1.108759\n",
            "\n",
            "Test set: Average loss: 0.4640, Accuracy: 8825/10000 (88%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.605651\n",
            "Train Epoch: 3 [5000/60000 (8%)]\tLoss: 2.315845\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 2.302563\n",
            "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 2.333453\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 2.296792\n",
            "Train Epoch: 3 [25000/60000 (42%)]\tLoss: 2.307289\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 2.325564\n",
            "Train Epoch: 3 [35000/60000 (58%)]\tLoss: 2.300469\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 2.282709\n",
            "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 2.307159\n",
            "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 2.261706\n",
            "Train Epoch: 3 [55000/60000 (92%)]\tLoss: 2.315381\n",
            "\n",
            "Test set: Average loss: 2.3011, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.284978\n",
            "Train Epoch: 4 [5000/60000 (8%)]\tLoss: 2.297261\n",
            "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 2.246850\n",
            "Train Epoch: 4 [15000/60000 (25%)]\tLoss: 2.290290\n",
            "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 2.309348\n",
            "Train Epoch: 4 [25000/60000 (42%)]\tLoss: 2.308997\n",
            "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 2.341841\n",
            "Train Epoch: 4 [35000/60000 (58%)]\tLoss: 2.323600\n",
            "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 2.293937\n",
            "Train Epoch: 4 [45000/60000 (75%)]\tLoss: 2.274011\n",
            "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 2.323820\n",
            "Train Epoch: 4 [55000/60000 (92%)]\tLoss: 2.325757\n",
            "\n",
            "Test set: Average loss: 2.3010, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.293566\n",
            "Train Epoch: 5 [5000/60000 (8%)]\tLoss: 2.290231\n",
            "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 2.324976\n",
            "Train Epoch: 5 [15000/60000 (25%)]\tLoss: 2.322768\n",
            "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 2.310856\n",
            "Train Epoch: 5 [25000/60000 (42%)]\tLoss: 2.278534\n",
            "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 2.296467\n",
            "Train Epoch: 5 [35000/60000 (58%)]\tLoss: 2.285993\n",
            "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 2.290549\n",
            "Train Epoch: 5 [45000/60000 (75%)]\tLoss: 2.299711\n",
            "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 2.307783\n",
            "Train Epoch: 5 [55000/60000 (92%)]\tLoss: 2.298001\n",
            "\n",
            "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.276439\n",
            "Train Epoch: 6 [5000/60000 (8%)]\tLoss: 2.318710\n",
            "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 2.304714\n",
            "Train Epoch: 6 [15000/60000 (25%)]\tLoss: 2.315247\n",
            "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 2.316167\n",
            "Train Epoch: 6 [25000/60000 (42%)]\tLoss: 2.291372\n",
            "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 2.304308\n",
            "Train Epoch: 6 [35000/60000 (58%)]\tLoss: 2.311361\n",
            "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 2.313739\n",
            "Train Epoch: 6 [45000/60000 (75%)]\tLoss: 2.293923\n",
            "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 2.302930\n",
            "Train Epoch: 6 [55000/60000 (92%)]\tLoss: 2.295053\n",
            "\n",
            "Test set: Average loss: 2.3011, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.309732\n",
            "Train Epoch: 7 [5000/60000 (8%)]\tLoss: 2.304738\n",
            "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 2.300414\n",
            "Train Epoch: 7 [15000/60000 (25%)]\tLoss: 2.310754\n",
            "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 2.331042\n",
            "Train Epoch: 7 [25000/60000 (42%)]\tLoss: 2.298365\n",
            "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 2.292065\n",
            "Train Epoch: 7 [35000/60000 (58%)]\tLoss: 2.292199\n",
            "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 2.307622\n",
            "Train Epoch: 7 [45000/60000 (75%)]\tLoss: 2.307882\n",
            "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 2.306256\n",
            "Train Epoch: 7 [55000/60000 (92%)]\tLoss: 2.272171\n",
            "\n",
            "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.298778\n",
            "Train Epoch: 8 [5000/60000 (8%)]\tLoss: 2.301926\n",
            "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 2.295218\n",
            "Train Epoch: 8 [15000/60000 (25%)]\tLoss: 2.276527\n",
            "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 2.319008\n",
            "Train Epoch: 8 [25000/60000 (42%)]\tLoss: 2.308945\n",
            "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 2.273895\n",
            "Train Epoch: 8 [35000/60000 (58%)]\tLoss: 2.309884\n",
            "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 2.321687\n",
            "Train Epoch: 8 [45000/60000 (75%)]\tLoss: 2.310383\n",
            "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 2.330701\n",
            "Train Epoch: 8 [55000/60000 (92%)]\tLoss: 2.311516\n",
            "\n",
            "Test set: Average loss: 2.3011, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.280082\n",
            "Train Epoch: 9 [5000/60000 (8%)]\tLoss: 2.362041\n",
            "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 2.298718\n",
            "Train Epoch: 9 [15000/60000 (25%)]\tLoss: 2.253270\n",
            "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 2.288388\n",
            "Train Epoch: 9 [25000/60000 (42%)]\tLoss: 2.244986\n",
            "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 2.319584\n",
            "Train Epoch: 9 [35000/60000 (58%)]\tLoss: 2.300009\n",
            "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 2.305882\n",
            "Train Epoch: 9 [45000/60000 (75%)]\tLoss: 2.294458\n",
            "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 2.288064\n",
            "Train Epoch: 9 [55000/60000 (92%)]\tLoss: 2.296369\n",
            "\n",
            "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.296080\n",
            "Train Epoch: 10 [5000/60000 (8%)]\tLoss: 2.292124\n",
            "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 2.291588\n",
            "Train Epoch: 10 [15000/60000 (25%)]\tLoss: 2.304719\n",
            "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 2.272885\n",
            "Train Epoch: 10 [25000/60000 (42%)]\tLoss: 2.305636\n",
            "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 2.331054\n",
            "Train Epoch: 10 [35000/60000 (58%)]\tLoss: 2.300604\n",
            "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 2.304238\n",
            "Train Epoch: 10 [45000/60000 (75%)]\tLoss: 2.291048\n",
            "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 2.305911\n",
            "Train Epoch: 10 [55000/60000 (92%)]\tLoss: 2.304503\n",
            "\n",
            "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xe6kY13nPo6d",
        "colab_type": "code",
        "outputId": "fa9d0bee-4802-4af4-a85b-1bb90919d261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2922
        }
      },
      "cell_type": "code",
      "source": [
        "args = {}\n",
        "for epoch in range(1, 10 + 1):\n",
        "    train(args, model, device, train_loader1, optimizer, epoch)\n",
        "    test(args, model, device, test_loader1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.253921\n",
            "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.960461\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.720668\n",
            "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.371742\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.367235\n",
            "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.285541\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.038327\n",
            "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.368944\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.115246\n",
            "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.252454\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.020829\n",
            "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 1.113088\n",
            "\n",
            "Test set: Average loss: 0.0650, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.275072\n",
            "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.069032\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.243955\n",
            "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.073269\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.069016\n",
            "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.130353\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.275998\n",
            "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.189612\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.092609\n",
            "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.061116\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.189856\n",
            "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.372821\n",
            "\n",
            "Test set: Average loss: 0.0555, Accuracy: 9819/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.026164\n",
            "Train Epoch: 3 [5000/60000 (8%)]\tLoss: 0.059945\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.038118\n",
            "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 0.032979\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.034029\n",
            "Train Epoch: 3 [25000/60000 (42%)]\tLoss: 0.611610\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.548707\n",
            "Train Epoch: 3 [35000/60000 (58%)]\tLoss: 0.028693\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.071589\n",
            "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 0.093905\n",
            "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.000210\n",
            "Train Epoch: 3 [55000/60000 (92%)]\tLoss: 0.004159\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 9836/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.646301\n",
            "Train Epoch: 4 [5000/60000 (8%)]\tLoss: 0.052432\n",
            "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.046392\n",
            "Train Epoch: 4 [15000/60000 (25%)]\tLoss: 0.403397\n",
            "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.385717\n",
            "Train Epoch: 4 [25000/60000 (42%)]\tLoss: 0.358198\n",
            "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.000856\n",
            "Train Epoch: 4 [35000/60000 (58%)]\tLoss: 0.557512\n",
            "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.039386\n",
            "Train Epoch: 4 [45000/60000 (75%)]\tLoss: 0.127620\n",
            "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.025258\n",
            "Train Epoch: 4 [55000/60000 (92%)]\tLoss: 0.009046\n",
            "\n",
            "Test set: Average loss: 0.0500, Accuracy: 9838/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.088403\n",
            "Train Epoch: 5 [5000/60000 (8%)]\tLoss: 0.033645\n",
            "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.691406\n",
            "Train Epoch: 5 [15000/60000 (25%)]\tLoss: 0.020585\n",
            "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.263211\n",
            "Train Epoch: 5 [25000/60000 (42%)]\tLoss: 0.066764\n",
            "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.003864\n",
            "Train Epoch: 5 [35000/60000 (58%)]\tLoss: 0.010614\n",
            "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.017625\n",
            "Train Epoch: 5 [45000/60000 (75%)]\tLoss: 0.088543\n",
            "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.720586\n",
            "Train Epoch: 5 [55000/60000 (92%)]\tLoss: 0.454467\n",
            "\n",
            "Test set: Average loss: 0.0430, Accuracy: 9871/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.024226\n",
            "Train Epoch: 6 [5000/60000 (8%)]\tLoss: 0.813029\n",
            "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.056640\n",
            "Train Epoch: 6 [15000/60000 (25%)]\tLoss: 0.003267\n",
            "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.181458\n",
            "Train Epoch: 6 [25000/60000 (42%)]\tLoss: 0.190644\n",
            "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.804603\n",
            "Train Epoch: 6 [35000/60000 (58%)]\tLoss: 0.168440\n",
            "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.534652\n",
            "Train Epoch: 6 [45000/60000 (75%)]\tLoss: 0.029592\n",
            "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.013996\n",
            "Train Epoch: 6 [55000/60000 (92%)]\tLoss: 0.145624\n",
            "\n",
            "Test set: Average loss: 0.0413, Accuracy: 9878/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.211312\n",
            "Train Epoch: 7 [5000/60000 (8%)]\tLoss: 0.633367\n",
            "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.167328\n",
            "Train Epoch: 7 [15000/60000 (25%)]\tLoss: 0.199522\n",
            "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.041588\n",
            "Train Epoch: 7 [25000/60000 (42%)]\tLoss: 0.044960\n",
            "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.130429\n",
            "Train Epoch: 7 [35000/60000 (58%)]\tLoss: 0.013398\n",
            "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.000478\n",
            "Train Epoch: 7 [45000/60000 (75%)]\tLoss: 0.158750\n",
            "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.273330\n",
            "Train Epoch: 7 [55000/60000 (92%)]\tLoss: 0.002111\n",
            "\n",
            "Test set: Average loss: 0.0403, Accuracy: 9881/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.276805\n",
            "Train Epoch: 8 [5000/60000 (8%)]\tLoss: 0.124270\n",
            "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.000376\n",
            "Train Epoch: 8 [15000/60000 (25%)]\tLoss: 0.262020\n",
            "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.008416\n",
            "Train Epoch: 8 [25000/60000 (42%)]\tLoss: 0.014057\n",
            "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.023905\n",
            "Train Epoch: 8 [35000/60000 (58%)]\tLoss: 0.054277\n",
            "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.025848\n",
            "Train Epoch: 8 [45000/60000 (75%)]\tLoss: 0.026731\n",
            "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.028638\n",
            "Train Epoch: 8 [55000/60000 (92%)]\tLoss: 0.143108\n",
            "\n",
            "Test set: Average loss: 0.0398, Accuracy: 9884/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.197875\n",
            "Train Epoch: 9 [5000/60000 (8%)]\tLoss: 0.459691\n",
            "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.213727\n",
            "Train Epoch: 9 [15000/60000 (25%)]\tLoss: 0.099714\n",
            "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.003115\n",
            "Train Epoch: 9 [25000/60000 (42%)]\tLoss: 0.808426\n",
            "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.054375\n",
            "Train Epoch: 9 [35000/60000 (58%)]\tLoss: 0.013683\n",
            "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.335518\n",
            "Train Epoch: 9 [45000/60000 (75%)]\tLoss: 0.004804\n",
            "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.021880\n",
            "Train Epoch: 9 [55000/60000 (92%)]\tLoss: 0.006525\n",
            "\n",
            "Test set: Average loss: 0.0428, Accuracy: 9869/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.002121\n",
            "Train Epoch: 10 [5000/60000 (8%)]\tLoss: 0.414039\n",
            "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.152271\n",
            "Train Epoch: 10 [15000/60000 (25%)]\tLoss: 0.987513\n",
            "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.002624\n",
            "Train Epoch: 10 [25000/60000 (42%)]\tLoss: 0.023845\n",
            "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.019326\n",
            "Train Epoch: 10 [35000/60000 (58%)]\tLoss: 0.042739\n",
            "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.070149\n",
            "Train Epoch: 10 [45000/60000 (75%)]\tLoss: 0.402205\n",
            "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.387941\n",
            "Train Epoch: 10 [55000/60000 (92%)]\tLoss: 0.030183\n",
            "\n",
            "Test set: Average loss: 0.0439, Accuracy: 9873/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xWOgXagVd8L8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}